# Visualizing Activations in Semantic Segmentation Networks
**Connor Cook, Matt Hanley, Basu Parmar, Galen Pogoncheff, Scott Young**

[![Build Status](https://travis-ci.com/matthewdhanley/csci5922.svg?branch=master)](https://travis-ci.com/matthewdhanley/csci5922)

#### Contents
- [Project Overview](#overview)
- [Comparison Methods](#analysis)
  * [Visualizing Activations](#activ)
  * [Visualizing Maximally Activating Images](#max_activ)
- [Summary of Results](#results)
- [Getting Started](#start)
- [Setup](#setup)
  * [Dependencies](#dependencies)
  * [Project Datasets](#data)
- [Training](#training)
- [Visualizing Activations](#activations)
  * [Save Activations](#activations_save)
  * [Channel Matching](#channel_match)
  * [View Activations](#activations_view)
- [Visualizing Convolutional Channels](#channel_vis)
- [References](#references)

<a name="overview"></a>
## Project Overview
Neural networks are notoriously known for being difficult to interpret. The visualization techniques proposed by [Zeiler and Fergus (2013)](https://arxiv.org/abs/1311.2901) for visualizing the activity within convolutional networks gave rise to a deeper understanding of what is being learned by convolutional networks, enabling a more methodical model development process. Despite the success of these visualization techniques on convolutional networks designed for image classification, similar approaches are not widely used in networks constructed for semantic segmentation.<br>

This project aims to address this problem by comparing visualizations from networks trained for image classification and networks trained for semantic segmentation.  These comparisions were used to conclude whether the task of semantic segmentaion leads the encoder portion of the network to learn inherently different features than it would if the network were to be trained for image classification.<br>

The two networks that we used for this analysis were a VGG-11 classification network pretrained on the imagenet dataset and an implementation of [TernausNet](https://arxiv.org/abs/1801.05746), a U-Net which features an encoder with the VGG-11 architecture.  We trained the U-Net on the [Cityscapes](https://www.cityscapes-dataset.com) semantic segmentation dataset.

<a name="analysis"></a>
## Comparison Methods
Two visualization methods were used to qualitatively analyze the differences in the encoder layers of the classification and semantic segmentation networks - visualizing the activations of each layer after forwarding an image through the network and artificially reconstructing an image that maximally activates an output channel of a convolutional layer.

<a name="activ"></a>
### Visualizing Activations
Activation visualizations were generated by forwarding an image (sourced from the Cityscapes or Tiny ImageNet dataset) through each network, saving the activation values from every channel of each layer of the network, and then projecting these activations into the input pixel space.  In order to aid in the visualization of these results, image heatmaps were created by highlighting the activated regions on the original image.  Provided below are two sample visualizations.

***Figure 1**: Activations from Layer 2 Channel 5.  Left Column: VGG-11 Channel Activations (top), heatmap of activations over cityscapes input image (middle), difference between VGG-11 channel activations and associated U-Net channel activations (bottom). Right Column: U-Net Channel Activations (top), heatmap of activations over cityscapes input image (middle), difference between U-Net channel activations and associated VGG-11 channel activations (bottom)*

<p align="center">
 <img src="images/activations_cityscape.png" width=1024>
</p>

***Figure 2**: Activations from Layer 12 Channel 58.  Left Column: VGG-11 Channel Activations (top), heatmap of activations over Tiny ImageNet input image (middle), difference between VGG-11 channel activations and associated U-Net channel activations (bottom). Right Column: U-Net Channel Activations (top), heatmap of activations over Tiny ImageNet input image (middle), difference between U-Net channel activations and associated VGG-11 channel activations (bottom)*

<p align="center">
 <img src="images/activations_tinyimagenet.png" width=1024>
</p>

<a name="max_activ"></a>
### Visualizing Maximally Activating Images
Another method for visualizing the mechanics of a convolutional network is to construct an image that maximally activates an output channel of a given convolutional encoding layer.  A parallel idea has been previously explored in the works of [DeepDream](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html).  The resulting constructed input image allowed us to view the patterns and textures that the network is seeking to find in a given layerâ€™s output channel.

The process of solving for this maximally activating image involved starting with an image with randomized RGB pixel values and then iteratively upscaling and optimizing the pixel values of this image.  In each iteration of this process, we first update the pixel values of the input image for a set number of steps.  This optimization process entails maximizing the mean activation of the selected output channel of the convolutional layer.  We therefore define a loss function as the negation of the mean value of the output feature map.  We minimize this loss function by propagating the loss backwards to the input image (while keeping the network parameters fixed), enabling us to progressively construct an input image that maximizes this feature map.  Following this optimization step, we upscale the image size before proceeding to the subsequent iteration.  After performing multiple iterations, the resulting input image closely approximates the image that maximizes the feature map in question.

Provided below are sample visualiations that accomplish this task.

***Figure 3**: Images that maximally activate 9 different channels of convolutional layer 2 of the VGG-11 classification encoder*
<p align="center">
 <img src="images/vgg11_layer2_channels81-74-2-30-7-19-122-100-83.png" width=512>
</p>

***Figure 4**: Images that maximally activate 9 different channels of convolutional layer 2 of the U-Net semantic segmentation encoder*
<p align="center">
 <img src="images/unet_layer2_channels101-23-13-109-99-78-39-18-30.png" width=512>
</p>

***Figure 5**: Images that maximally activate 9 different channels of convolutional layer 7 of the VGG-11 classification encoder*
<p align="center">
 <img src="images/vgg11_layer7_channels211-48-368-11-43-148-405-187-178.png" width=512>
</p>

***Figure 6**: Images that maximally activate 9 different channels of convolutional layer 7 of the U-Net semantic segmentation encoder*
<p align="center">
 <img src="images/unet_layer7_channels68-384-488-365-51-484-118-46-317.png" width=512>
</p>

<a name="results"></a>
## Summary of Results
In early convolutional layers, the channels of the VGG-11 and the U-Net encoders learned similar features (simple shapes and patterns such as edges and corners).  This observation was supported by the edge and corner patterns displayed in the activation visualizations and the simple striations depicted in the maximally activating images.  Proceeding the convolutional filters learned in the early layers, visualizations from layers deeper in the two encoders revealed that the two networks learned distinctly different features.  We observed that the VGG-11 filters (from the classification network) specialize in detecting distinct features that help identify subsets of image classes, whereas the U-Net filters appeared to learn a blurred representation of the image thus preserving details of the original image.  This was most noticeable while inspecting the images that maximally activate channels of deeper layers of the encoder.  The images that maximally activate the channels of the VGG-11 encoder yield insight into the class of the object featured in the image (*Figure 7*). This coherency is not immediately observable in the images that maximally activate the corresponding layer of the U-Net encoder (*Figure 8*).  Given these findings, we arrived at the conclusion that the task of semantic segmentation leads the encoder to learn inherently different features from those learned when the same architecture is trained for image classification.

As an additional experiment, we attempted to test whether an encoder trained for image classification can be substituted into the encoder portion of a fully convolutional network for semantic segmentation.  In order to accomplish this task, we created a U-Net model that featured a VGG-11 encoder (pre-trained on ImageNet) and then trained the network for semantic segmentation on the Cityscapes dataset while keeping the encoder's weights fixed.  The performances of this network and a U-Net (with the same architecure) trained from scratch were finally compared.  The performance of these networks on the Cityscapes validation data are provided below in Table 1.  Interestingly, the two networks offered very similar performances.  Given these results and a limited evaluation, we were unable to confidently determine if this encoder substitution had a significant impact on the network's performance.

***Table 1**: Comparisons of segmentation performance on Cityscaped validation data.*

|                  | In Situ Model | Pre-trained Encoder Model | Untrained Model |
|------------------|---------------|---------------------------|-----------------|
| Overall Accuracy |    84.90%     |        84.25%             |    66.33%       |
| Mean Recall      |    38.56%     |        38.00%             |    17.62%       |
| Mean IoU         |    31.39%     |        30.90%             |    13.62%       |


<a name="start"></a>
## Getting Started
Usage can be obtained by running
```
python main.py --help
```

1. It is suggested that dependencies are installed in a virtual environment for this project. Installing requirements can
be done as described in [Dependencies](#dependencies).
2. Download the Cityscapes dataset from the link in the [Data](#data) section. Once it is downloaded, ensure that the
structure matches that defined in [Cityscapes dataset directory structure](#cs-structure).
3. Train the in situ UNet model and pretrained VGG-11 encoder UNet model as described in the [Training](#training) section.
4. Get activations from the trained networks as specified in [Save Activations](#save-activations).
5. Match channels of the activations as described in [Channel Matching](#channel-matching).

*Note:* when running the above training or activation saving, the keyword `--subset` can be used to use a small subset
of cityscape images. Without sufficient resources, these tasks can be very slow (or even impossible). The keyword
`--no_resize` can be used to not scale down the images, however this would result in a very computationally expensive
process. It is not feasible to do this with the entire dataset unless you have a lot of RAM.

Now you are set up to perform a number of tasks:
* Visualize activations of channels as described in [View Activations](#view-activations).
* View maximally activated input images as described in [Visualizing Maximally Activating Images](#max_activ)
* Check performance of model as described in [Validating models and measuring performance](#validation)

<a name="dependencies"></a>
### Dependencies
This source code for this project was written in Python 3.  The following command will install the required project dependencies. Unit testing is performed for Python versions 3.5 and 3.6.
```
pip install -r requirements.txt
```

<a name="data"></a>
### Project Datasets
We used the [Cityscapes](https://www.cityscapes-dataset.com/downloads/) dataset for semantic segmentaion (fine 
annotations) to train our U-Net model.  The dataset is comprised of 3475 labeled training and validation images, combined,
and 1525 test images. It is possible to also get an additional 19,998 coarsely annotated images, but we were limited by
computational resources to process this much data.

Network activation visualizations were obtained using images from the Cityscapes dataset and the 
[Tiny ImageNet](https://tiny-imagenet.herokuapp.com/) dataset.

<a name="cs-structure"></a>
#### Cityscapes dataset directory structure
Shown below is the Cityscape data hierarchy expected by our training script.  Within each of the test, train, and val 
folders are folders named after the city that the data was taken. These folders hold the respective data.
```
.
â”œâ”€â”€ gtFine
â”‚Â Â  â”œâ”€â”€ test
â”‚Â Â  â”œâ”€â”€ train
â”‚Â Â  â””â”€â”€ val
â””â”€â”€ leftImg8bit
    â”œâ”€â”€ test
    â”œâ”€â”€ train
    â””â”€â”€ val
```

<a name="training"></a>
## Training
In situ model training:
```
python main.py cityscapes_path/ --mode train --savedir checkpoints/ --file unet.tar
```

Training with pretrained VGG-11 encoder:
```
python main.py cityscapes_path/ --mode train --savedir checkpoints/ --file unet.tar --pretrained
```
This latter configuration loads the weights for the VGG-11 encoder that was trained on ImageNet then freezes those layers
as it trains.

When training, the user has the option to change some hyperparameters from the command line interface. Options that can
be changed include batch size `--batch_size` (default 8), learning rate `--learning_rate` (default 0.001), or number of epochs `--epochs` (default 100).
The analysis done here used all the default parameters.

<a name="activations"></a>
## Visualizing Activations

<a name="activations_save"></a>
#### Save Activations
```
python main.py path/to/dataset --mode activations --model unet --checkpoint unet.tar
```
This will create a directory from the root of the project called `activations/` and will create a UNet activations file if
run with `--model unet` and a VGGmod activations file if run with argument `--model vggmod`. In order to run channel
matching, both models must be run. This can be in one fell swoop done with `--model both`.

<a name="channel_match"></a>
#### Channel Matching
```
python main.py path/to/activations --mode compare_activations --type normal
```
This will try to best match the channels based on their activations. This will output a file called UNet_activations_matched 
which is essentially a rearranged version of the activations file created by `--mode activations`. `--type` specifies 
how the channel acivations are preprocessed before the channel matching is performed. Values can be 'normal', 'blur', 'dilation', or 'pooling'.

<a name="activations_view"></a>
#### View Activations
```
python main.py path/to/activations --mode view_activations
```
This will create a folder called `activation_visualizations` and will place photos showing activations for both VGG-11
and UNet. Can use parameters `--start_layer` and `--stop_layer` to specify which layer to start and stop with visualizations.

Viewing activations provides a grid of size 3x2 images. The left column images are derived from the VGG-11 classifier. 
The right column of images are derived from the in situ UNet. The first row is the activation of the channel visualized
as a grayscale image. The second row shows the activations as a heat map overlaid on the original image. This allows us
to see what features the filter is looking for in that specific channel and layer. The final row shows the difference
between the VGG-11 activations and in situ UNet activations. The left will be VGG-11 minus UNet, clipping values below
zero. The right is the opposite, also clipping values less than zero.

<a name="channel_vis"></a>
## Visualizing Convolutional Channels
Save the image that maximally activates output channel 23 of convolutional layer 3 of the U-Net encoder:
```
python main.py path/to/save/output --mode view_max_activating --model unet --checkpoint unet.tar --conv_layer 2 --channels 23 -lr 0.01 --verbose
```

Save the image that maximally activates output channel 168 of convolutional layer 7 of the VGG-11 encoder:
```
python main.py path/to/save/output --mode view_max_activating --model vggmod --conv_layer 7 --channels 168 -lr 0.01 --verbose
```

Save a 3x3 grid of images that maximally activate 9 output channels of convolutional layer 8 of the VGG-11 encoder:
```
python main.py path/to/save/output --mode view_max_activating --model vggmod --conv_layer 8 -lr 0.01 --grid --verbose
```

These commands will save a .png file of the image that maximally activates the specified channels of a given convolutional layer.  The convolutional layer and channel to be visualized can be specified with the ```conv_layer``` and ```channels``` arguments, respectively.  Specifying the ```--grid``` argument will generated a 3x3 array of images, each of which depicts an image that maximally activates an output channel of the specified layer.  The ```--verbose``` argument may be excluded from the command if you do not wish for status updates to be output.

<a name="validation"></a>
## Validating models and measuring performance
```
python main.py path/to/dataset --mode test --checkpoint model.tar
```

<a name="references"></a>
## References
TODO
