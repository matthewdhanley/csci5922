# Visualizing Activations in Semantic Segmentation Networks
**Connor Cook, Matt Hanley, Basu Parmar, Galen Pogoncheff, Scott Young**

[![Build Status](https://travis-ci.com/matthewdhanley/csci5922.svg?branch=master)](https://travis-ci.com/matthewdhanley/csci5922)

#### Contents
- [Project Overview](#overview)
- [Comparison Methods](#analysis)
  * [Visualizing Activations](#activ)
  * [Visualizing Maximally Activating Images](#max_activ)
- [Summary of Results](#results)
- [Getting Started](#start)
- [Setup](#setup)
  * [Dependencies](#dependencies)
  * [Project Datasets](#data)
- [Training](#training)
- [Visualizing Activations](#activations)
  * [Save Activations](#activations_save)
  * [Channel Matching](#channel_match)
  * [View Activations](#activations_view)
- [Visualizing Convolutional Channels](#channel_vis)

<a name="overview"></a>
## Project Overview
Neural networks are notoriously known for being difficult to interpret. The visualization techniques proposed by [Zeiler and Fergus (2013)](https://arxiv.org/abs/1311.2901) for visualizing the activity within convolutional networks gave rise to a deeper understanding of what is being learned by convolutional networks, enabling a more methodical model development process. Despite the success of these visualization techniques on convolutional networks designed for image classification, similar approaches are not widely used in networks constructed for semantic segmentation.<br>

This project aims to address this problem by comparing visualizations from networks trained for image classification and networks trained for semantic segmentation.  These comparisions were then used to conclude whether the task of semantic segmentaion leads the encoder portion of the network to learn inherently different features than it would if the network to be trained for image classification.<br>

The two networks that we used for this analysis were a VGG-11 classification network pretrained on the imagenet dataset and an implementation of [TernausNet](https://arxiv.org/abs/1801.05746), a U-Net which features a VGG-11 encoder.  We trained the U-Net on the [Cityscapes](https://www.cityscapes-dataset.com) semantic segmentation dataset.

<a name="analysis"></a>
## Comparison Methods
Two visualization methods were used to qualitatively analyze the differences in the encoder layers of the classification and semantic segmentation networks - visualizing the activations of each layer after forwarding an image throught the network and artificially reconstructing an image that maximally activates an output channel of a convolutional layer.

<a name="activ"></a>
### Visualizing Activations
Activation visualizations were generated by forwarding an image (sourced from the Cityscapes or Tiny ImageNet dataset) through each network, saving the activations values from every channel of each layer of the network, and then projecting these activations into the input pixel space.  In order to aid in the visualization of these results, image heatmaps were created by highlighting the activated regions of the original image.  Provided below are two sample visualizations.

*Figure 1: Activations from Layer 1 Channel 5.  Left Column: VGG-11 Channel Activations (top), heatmap of activations over cityscapes input image (middle), difference between VGG-11 channel activations and associated U-Net channel activations (bottom). Right Column: U-Net Channel Activations (top), heatmap of activations over cityscapes input image (middle), difference between U-Net channel activations and associated VGG-11 channel activations (bottom)*
<img src="images/activations_cityscape.png" width=1024>

*Figure 2: Activations from Layer 12 Channel 58.  Left Column: VGG-11 Channel Activations (top), heatmap of activations over Tiny ImageNet input image (middle), difference between VGG-11 channel activations and associated U-Net channel activations (bottom). Right Column: U-Net Channel Activations (top), heatmap of activations over Tiny ImageNet input image (middle), difference between U-Net channel activations and associated VGG-11 channel activations (bottom)*
<img src="images/activations_tinyimagenet.png" width=1024>

<a name="max_activ"></a>
### Visualizing Maximally Activating Images

<a name="results"></a>
## Summary of Results

<a name="start"></a>
## Getting Started
1. It is suggested that dependencies are installed in a virtual environment for this project. Installing requirements can
be done as described in [Dependencies](#dependencies).
2. Download the Cityscapes dataset from the link in the [Data](#data) section. Once it is downloaded, ensure that the
structure matches that defined in [Cityscapes dataset directory structure](#cs-structure).
3. Train the in situ UNet model and pretrained VGG-11 encoder UNet model as described in the [Training](#training) section.
4. Get activations from the trained networks as specified in [Save Activations](#save-activations).
5. Match channels of the activations as described in [Channel Matching](#channel-matching).

Now you are setup to perform a number of tasks:
1. Visualize activations of channels as described in [View Activations](#view-activations).
2. View maximally activated input images as described in [Visualizing Maximally Activating Images](#max_activ)
3. Check performance of model as described in [Validating models and measuring performance](#validation)

<a name="dependencies"></a>
### Dependencies
This source code for this project was written in Python 3.  The following command will install the required project dependecies.
```
pip install -r requirements.txt
```

<a name="data"></a>
### Project Datasets
We used the [Cityscapes](https://www.cityscapes-dataset.com/downloads/) dataset for semantic segmentaion (fine 
annotations) to train our U-Net model.  Network activation visualizations were obtained using images from Cityscapes 
dataset and the [Tiny ImageNet](https://tiny-imagenet.herokuapp.com/) dataset.

<a name="cs-structure"></a>
#### Cityscapes dataset directory structure
Shown below is the Cityscape data hierarchy expectedby our training script.  Within each of the test, train, and val 
folders are folders named after the city that the data was taken. These folders hold the respective data.
```
.
├── gtFine
│   ├── test
│   ├── train
│   └── val
└── leftImg8bit
    ├── test
    ├── train
    └── val
```

<a name="training"></a>
## Training
In situ model training:
```
python main.py cityscapes_path/ --mode train --save_dir checkpoints/ --file unet.tar
```

Training with pretrained VGG-11 encoder:
```
python main.py cityscapes_path/ --mode train --save_dir checkpoints/ --file unet.tar --pretrained
```
This latter configuration loads the weights for the VGG-11 encoder that was trained on ImageNet then freezes those layers
as it trains.

<a name="activations"></a>
## Visualizing Activations

<a name="activations_save"></a>
#### Save Activations
```
python main.py path/to/dataset --mode activations --model unet --checkpoint unet.tar
```
This will create a directory from the root of the project called `activations/` and will create a UNet activations file if
run with `--model unet` and a VGGmod activations file if run with argument `--model vggmod`. In order to run channel
matching, both models must be run. This can be in one fell swoop done with `--model both`.

<a name="channel_match"></a>
#### Channel Matching
```
python main.py path/to/activations --mode compare_activations --type normal
```
This will try to best match the channels based on their activations. This will output a file called UNet_activations_matched 
which is essentially a rearranged version of the activations file created by `--mode activations`. `--type` specifies 
how the channel matching is performed. Values can be 'normal', 'blur', 'dilation', or 'pooling'.

<a name="activations_view"></a>
#### View Activations
```
python main.py path/to/activations --mode view_activations
```
This will create a folder called `activation_visuaalizations` and will place photos showing activations for both VGG-11
and UNet. Can use parameters `--start_layer` and `--stop_layer` to specify which layer to start and stop with visualizations.

<a name="channel_vis"></a>
## Visualizing Convolutional Channels
```
python main.py path --mode ... #TODO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
```

<a name="validation"></a>
## Validating models and measuring performance
```
python main.py path/to/dataset --mode test --checkpoint model.tar
```
